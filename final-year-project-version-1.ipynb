{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, TensorDataset\n# from torch.cuda.amp import GradScaler, autocast\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n# from sklearn.neural_network import MLPClassifier\n# import xgboost as xgb\n# import matplotlib.pyplot as plt\n# from tqdm import tqdm\n# import numpy as np\n\n# # ==========================\n# #        PARAMETERS\n# # ==========================\n# data_path = '/kaggle/input/creditcardfraud/creditcard.csv'  # Replace with your dataset path\n# input_dim = 30  # Number of features\n# latent_dim = 100\n# batch_size = 256  # Increased batch size for GPU utilization\n# epochs = 3000\n# learning_rate = 0.0003\n# betas = (0.5, 0.999)\n\n# # ==========================\n# #    DEVICE CONFIGURATION\n# # ==========================\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {device}\")\n\n# # ==========================\n# #      DATA PREPARATION\n# # ==========================\n# def load_data(path):\n#     data = pd.read_csv(path)\n#     return data\n\n# def preprocess_data(data):\n#     # Handle missing values\n#     data = data.dropna()\n    \n#     # Features and target\n#     X = data.drop('Class', axis=1)\n#     y = data['Class']\n    \n#     # Normalize features\n#     scaler = StandardScaler()\n#     X_scaled = scaler.fit_transform(X)\n    \n#     return X_scaled, y\n\n# def split_data(X, y, test_size=0.2, random_state=42):\n#     X_train, X_test, y_train, y_test = train_test_split(\n#         X, y, test_size=test_size, random_state=random_state, stratify=y\n#     )\n#     return X_train, X_test, y_train, y_test\n\n# # Execute preprocessing\n# data = load_data(data_path)\n# X, y = preprocess_data(data)\n# X_train, X_test, y_train, y_test = split_data(X, y)\n\n# # ==========================\n# #      MODEL DEFINITIONS\n# # ==========================\n# # Improved Encoder\n# class ImprovedEncoder(nn.Module):\n#     def __init__(self, input_dim, latent_dim):\n#         super(ImprovedEncoder, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(input_dim, 2048),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, 1024),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, latent_dim * 2)  # Mean and log-variance\n#         )\n    \n#     def forward(self, x):\n#         h = self.fc(x)\n#         mu, logvar = torch.chunk(h, 2, dim=1)\n#         return mu, logvar\n\n# # Improved Decoder\n# class ImprovedDecoder(nn.Module):\n#     def __init__(self, latent_dim, output_dim):\n#         super(ImprovedDecoder, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(latent_dim, 1024),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, 2048),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, output_dim),\n#             nn.Sigmoid()\n#         )\n    \n#     def forward(self, z):\n#         return self.fc(z)\n\n# # Improved Discriminator\n# class ImprovedDiscriminator(nn.Module):\n#     def __init__(self, input_dim):\n#         super(ImprovedDiscriminator, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(input_dim, 2048),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, 1024),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, 1),\n#             nn.Sigmoid()\n#         )\n    \n#     def forward(self, x):\n#         return self.fc(x)\n\n# # Improved VAEGAN Model\n# class ImprovedVAEGAN(nn.Module):\n#     def __init__(self, input_dim, latent_dim):\n#         super(ImprovedVAEGAN, self).__init__()\n#         self.encoder = ImprovedEncoder(input_dim, latent_dim)\n#         self.decoder = ImprovedDecoder(latent_dim, input_dim)\n#         self.discriminator = ImprovedDiscriminator(input_dim)\n    \n#     def reparameterize(self, mu, logvar):\n#         std = torch.exp(0.5 * logvar)\n#         eps = torch.randn_like(std)\n#         return mu + eps * std\n    \n#     def forward(self, x):\n#         mu, logvar = self.encoder(x)\n#         z = self.reparameterize(mu, logvar)\n#         reconstructed = self.decoder(z)\n#         validity = self.discriminator(reconstructed)\n#         return reconstructed, validity, mu, logvar\n\n# # ==========================\n# #      HELPER FUNCTIONS\n# # ==========================\n# def get_submodule(model, submodule_name):\n#     \"\"\"\n#     Retrieve the submodule from the model, handling DataParallel if necessary.\n    \n#     Args:\n#         model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n#         submodule_name (str): Name of the submodule to retrieve.\n        \n#     Returns:\n#         torch.nn.Module: The requested submodule.\n#     \"\"\"\n#     if isinstance(model, nn.DataParallel):\n#         return getattr(model.module, submodule_name)\n#     else:\n#         return getattr(model, submodule_name)\n\n# def get_parameters(model, submodules):\n#     \"\"\"\n#     Retrieve the parameters from specified submodules.\n    \n#     Args:\n#         model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n#         submodules (list of str): List of submodule names whose parameters are to be retrieved.\n        \n#     Returns:\n#         list: List of parameters from the specified submodules.\n#     \"\"\"\n#     params = []\n#     for submodule in submodules:\n#         sub = get_submodule(model, submodule)\n#         params += list(sub.parameters())\n#     return params\n\n# # ==========================\n# #      MODEL INITIALIZATION\n# # ==========================\n# # Initialize the model\n# model = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n\n# # Optionally use DataParallel if multiple GPUs are available\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     model = nn.DataParallel(model)\n\n# print(\"Model is on GPU:\", next(model.parameters()).is_cuda)  # Debug statement\n# print(model)  # Print model architecture\n\n# # Function to count total parameters\n# def count_parameters(model):\n#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# total_params = count_parameters(model)\n# print(f\"Total trainable parameters: {total_params}\")\n\n# # ==========================\n# #        DATA LOADING\n# # ==========================\n# # Convert training data to tensors\n# train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n# train_loader = DataLoader(\n#     train_dataset,\n#     batch_size=batch_size,\n#     shuffle=True,\n#     pin_memory=True if device.type == 'cuda' else False,\n#     num_workers=4  # Adjust based on your CPU cores\n# )\n\n# # ==========================\n# #       OPTIMIZERS\n# # ==========================\n# # Initialize optimizers using helper functions\n# optimizer_G = optim.Adam(\n#     get_parameters(model, ['encoder', 'decoder']),\n#     lr=learning_rate, betas=betas\n# )\n# optimizer_D = optim.Adam(\n#     get_parameters(model, ['discriminator']),\n#     lr=learning_rate, betas=betas\n# )\n\n# # ==========================\n# #       LOSS FUNCTIONS\n# # ==========================\n# adversarial_loss = nn.MSELoss().to(device)\n# reconstruction_loss = nn.MSELoss().to(device)\n# kld_loss = lambda mu, logvar: -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n# # ==========================\n# #      MIXED PRECISION\n# # ==========================\n# scaler = GradScaler()\n\n# # ==========================\n# #      TRAINING LOOP\n# # ==========================\n# d_losses = []\n# g_losses = []\n\n# for epoch in tqdm(range(1, epochs + 1), desc=\"Training Epochs\"):\n#     epoch_d_loss = 0\n#     epoch_g_loss = 0\n#     model.train()\n    \n#     for batch_idx, (real_data, _) in enumerate(tqdm(train_loader, desc=\"Batches\", leave=False)):\n#         real_data = real_data.to(device)\n#         batch_size_current = real_data.size(0)\n        \n#         # ===================\n#         #   Train Discriminator\n#         # ===================\n#         optimizer_D.zero_grad()\n#         with autocast():  # Enable autocasting for mixed precision\n#             reconstructed, _, _, _ = model(real_data)\n#             valid = torch.ones(batch_size_current, 1, device=device)\n#             fake = torch.zeros(batch_size_current, 1, device=device)\n            \n#             # Use helper function to access discriminator\n#             discriminator = get_submodule(model, 'discriminator')\n#             real_discrim = discriminator(real_data)\n#             fake_discrim = discriminator(reconstructed.detach())\n            \n#             real_loss = adversarial_loss(real_discrim, valid)\n#             fake_loss = adversarial_loss(fake_discrim, fake)\n#             d_loss = (real_loss + fake_loss) / 2\n#         scaler.scale(d_loss).backward()\n#         scaler.step(optimizer_D)\n#         scaler.update()\n        \n#         # ===================\n#         #    Train Generator\n#         # ===================\n#         optimizer_G.zero_grad()\n#         with autocast():\n#             reconstructed, validity, mu, logvar = model(real_data)\n#             g_adv = adversarial_loss(validity, valid)\n#             g_recon = reconstruction_loss(reconstructed, real_data)\n#             g_kld = kld_loss(mu, logvar)\n#             g_loss = g_adv + g_recon + g_kld\n#         scaler.scale(g_loss).backward()\n#         scaler.step(optimizer_G)\n#         scaler.update()\n        \n#         # Accumulate losses\n#         epoch_d_loss += d_loss.item()\n#         epoch_g_loss += g_loss.item()\n    \n#     # Average losses for the epoch\n#     avg_d_loss = epoch_d_loss / len(train_loader)\n#     avg_g_loss = epoch_g_loss / len(train_loader)\n    \n#     d_losses.append(avg_d_loss)\n#     g_losses.append(avg_g_loss)\n    \n#     # Logging every 100 epochs\n#     if epoch % 100 == 0:\n#         print(f\"Epoch {epoch} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n\n# # ==========================\n# #        SAVE MODEL\n# # ==========================\n# # Save the model\n# if isinstance(model, nn.DataParallel):\n#     torch.save(model.module.state_dict(), 'improved_vae_gan.pth')\n# else:\n#     torch.save(model.state_dict(), 'improved_vae_gan.pth')\n\n# # ==========================\n# #        EVALUATION\n# # ==========================\n# # Parameters for Evaluation\n# batch_size_eval = 256  # Increased batch size\n\n# # Convert training data to tensors\n# train_dataset_eval = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n# train_loader_eval = DataLoader(\n#     train_dataset_eval,\n#     batch_size=batch_size_eval,\n#     shuffle=False,  # Typically, shuffle=False for evaluation\n#     pin_memory=True if device.type == 'cuda' else False,\n#     num_workers=4  # Adjust based on your CPU cores\n# )\n\n# # Load trained VAEGAN model and move to device\n# model_eval = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n# if isinstance(model, nn.DataParallel):\n#     model_eval.load_state_dict(torch.load('improved_vae_gan.pth'))\n# else:\n#     model_eval.load_state_dict(torch.load('improved_vae_gan.pth', map_location=device))\n# model_eval.eval()\n# print(\"Model loaded on GPU:\", next(model_eval.parameters()).is_cuda)  # Debug statement\n\n# # Generate synthetic data with tqdm\n# synthetic_data = []\n# with torch.no_grad():\n#     for batch_idx, (real_data, _) in enumerate(tqdm(train_loader_eval, desc=\"Generating Synthetic Data\")):\n#         real_data = real_data.to(device)\n#         mu, logvar = get_submodule(model_eval, 'encoder')(real_data)\n#         z = model_eval.reparameterize(mu, logvar)\n#         gen_data = get_submodule(model_eval, 'decoder')(z)\n#         synthetic_data.append(gen_data.cpu().numpy())\n# synthetic_data = np.vstack(synthetic_data)\n\n# # Combine real and synthetic data\n# X_augmented = np.vstack((X_train, synthetic_data))\n# y_augmented = np.hstack((y_train, np.ones(synthetic_data.shape[0])))\n\n# # Split augmented data\n# X_aug_train, X_aug_test, y_aug_train, y_aug_test = split_data(X_augmented, y_augmented)\n\n# # ==========================\n# #    CLASSIFICATION MODELS\n# # ==========================\n# # Train DNN Classifier\n# dnn = MLPClassifier(hidden_layer_sizes=(200,), max_iter=300, random_state=42)  # Increased hidden layers\n# dnn.fit(X_aug_train, y_aug_train)\n# y_pred_dnn = dnn.predict(X_test)\n\n# # Train XGBoost Classifier\n# xgb_model = xgb.XGBClassifier(\n#     use_label_encoder=False, \n#     eval_metric='logloss', \n#     n_estimators=100, \n#     max_depth=6, \n#     random_state=42\n# )\n# xgb_model.fit(X_train, y_train)\n# y_pred_xgb = xgb_model.predict(X_test)\n\n# # ==========================\n# #          METRICS\n# # ==========================\n# # Metrics for DNN\n# precision_dnn = precision_score(y_test, y_pred_dnn)\n# recall_dnn = recall_score(y_test, y_pred_dnn)\n# f1_dnn = f1_score(y_test, y_pred_dnn)\n\n# # Metrics for XGBoost\n# precision_xgb = precision_score(y_test, y_pred_xgb)\n# recall_xgb = recall_score(y_test, y_pred_xgb)\n# f1_xgb = f1_score(y_test, y_pred_xgb)\n\n# # Print Classification Reports\n# print(\"DNN Classification Report:\")\n# print(classification_report(y_test, y_pred_dnn))\n\n# print(\"XGBoost Classification Report:\")\n# print(classification_report(y_test, y_pred_xgb))\n\n# # Summary of Metrics\n# summary_metrics = {\n#     'DNN': {'Precision': precision_dnn, 'Recall': recall_dnn, 'F1-Score': f1_dnn},\n#     'XGBoost': {'Precision': precision_xgb, 'Recall': recall_xgb, 'F1-Score': f1_xgb}\n# }\n\n# print(\"Summary of Classification Metrics:\")\n# for model_name, metrics in summary_metrics.items():\n#     print(f\"{model_name}: Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, F1-Score={metrics['F1-Score']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Improved version of above \n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, TensorDataset\n# from torch.cuda.amp import GradScaler, autocast\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n# from sklearn.neural_network import MLPClassifier\n# import xgboost as xgb\n# import matplotlib.pyplot as plt\n# from tqdm import tqdm\n# import numpy as np\n\n# # ==========================\n# #        PARAMETERS\n# # ==========================\n# data_path = '/kaggle/input/creditcardfraud/creditcard.csv'  # Replace with your dataset path\n# input_dim = 30  # Number of features\n# latent_dim = 100\n# batch_size = 1024  # Increased batch size for GPU utilization\n# epochs = 3000\n# learning_rate = 0.0003\n# betas = (0.5, 0.999)\n\n# # ==========================\n# #    DEVICE CONFIGURATION\n# # ==========================\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# torch.backends.cudnn.benchmark = True  # Enable CuDNN benchmark for performance\n# print(f\"Using device: {device}\")\n\n# # ==========================\n# #      DATA PREPARATION\n# # ==========================\n# def load_data(path):\n#     data = pd.read_csv(path)\n#     return data\n\n# def preprocess_data(data):\n#     # Handle missing values\n#     data = data.dropna()\n    \n#     # Features and target\n#     X = data.drop('Class', axis=1)\n#     y = data['Class']\n    \n#     # Normalize features\n#     scaler = StandardScaler()\n#     X_scaled = scaler.fit_transform(X)\n    \n#     return X_scaled, y\n\n# def split_data(X, y, test_size=0.2, random_state=42):\n#     X_train, X_test, y_train, y_test = train_test_split(\n#         X, y, test_size=test_size, random_state=random_state, stratify=y\n#     )\n#     return X_train, X_test, y_train, y_test\n\n# # Execute preprocessing\n# data = load_data(data_path)\n# X, y = preprocess_data(data)\n# X_train, X_test, y_train, y_test = split_data(X, y)\n\n# # ==========================\n# #      MODEL DEFINITIONS\n# # ==========================\n# # Improved Encoder\n# class ImprovedEncoder(nn.Module):\n#     def __init__(self, input_dim, latent_dim):\n#         super(ImprovedEncoder, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(input_dim, 2048),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, 1024),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, latent_dim * 2)  # Mean and log-variance\n#         )\n    \n#     def forward(self, x):\n#         h = self.fc(x)\n#         mu, logvar = torch.chunk(h, 2, dim=1)\n#         return mu, logvar\n\n# # Improved Decoder\n# class ImprovedDecoder(nn.Module):\n#     def __init__(self, latent_dim, output_dim):\n#         super(ImprovedDecoder, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(latent_dim, 1024),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, 2048),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, output_dim),\n#             nn.Sigmoid()\n#         )\n    \n#     def forward(self, z):\n#         return self.fc(z)\n\n# # Improved Discriminator\n# class ImprovedDiscriminator(nn.Module):\n#     def __init__(self, input_dim):\n#         super(ImprovedDiscriminator, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(input_dim, 2048),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, 1024),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, 1),\n#             nn.Sigmoid()\n#         )\n    \n#     def forward(self, x):\n#         return self.fc(x)\n\n# # Improved VAEGAN Model\n# class ImprovedVAEGAN(nn.Module):\n#     def __init__(self, input_dim, latent_dim):\n#         super(ImprovedVAEGAN, self).__init__()\n#         self.encoder = ImprovedEncoder(input_dim, latent_dim)\n#         self.decoder = ImprovedDecoder(latent_dim, input_dim)\n#         self.discriminator = ImprovedDiscriminator(input_dim)\n    \n#     def reparameterize(self, mu, logvar):\n#         std = torch.exp(0.5 * logvar)\n#         eps = torch.randn_like(std)\n#         return mu + eps * std\n    \n#     def forward(self, x):\n#         mu, logvar = self.encoder(x)\n#         z = self.reparameterize(mu, logvar)\n#         reconstructed = self.decoder(z)\n#         validity = self.discriminator(reconstructed)\n#         return reconstructed, validity, mu, logvar\n\n# # ==========================\n# #      HELPER FUNCTIONS\n# # ==========================\n# def get_submodule(model, submodule_name):\n#     \"\"\"\n#     Retrieve the submodule from the model, handling DataParallel if necessary.\n    \n#     Args:\n#         model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n#         submodule_name (str): Name of the submodule to retrieve.\n        \n#     Returns:\n#         torch.nn.Module: The requested submodule.\n#     \"\"\"\n#     if isinstance(model, nn.DataParallel):\n#         return getattr(model.module, submodule_name)\n#     else:\n#         return getattr(model, submodule_name)\n\n# def get_parameters(model, submodules):\n#     \"\"\"\n#     Retrieve the parameters from specified submodules.\n    \n#     Args:\n#         model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n#         submodules (list of str): List of submodule names whose parameters are to be retrieved.\n        \n#     Returns:\n#         list: List of parameters from the specified submodules.\n#     \"\"\"\n#     params = []\n#     for submodule in submodules:\n#         sub = get_submodule(model, submodule)\n#         params += list(sub.parameters())\n#     return params\n\n# # ==========================\n# #      MODEL INITIALIZATION\n# # ==========================\n# # Initialize the model\n# model = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n\n# # Optionally use DataParallel if multiple GPUs are available\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     model = nn.DataParallel(model)\n\n# print(\"Model is on GPU:\", next(model.parameters()).is_cuda)  # Debug statement\n# print(model)  # Print model architecture\n\n# # Function to count total parameters\n# def count_parameters(model):\n#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# total_params = count_parameters(model)\n# print(f\"Total trainable parameters: {total_params}\")\n\n# # ==========================\n# #        DATA LOADING\n# # ==========================\n# # Convert training data to tensors\n# train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n# train_loader = DataLoader(\n#     train_dataset,\n#     batch_size=batch_size,\n#     shuffle=True,\n#     pin_memory=True if device.type == 'cuda' else False,\n#     num_workers=12  # Increased number of workers for faster data loading\n# )\n\n# # ==========================\n# #       OPTIMIZERS\n# # ==========================\n# # Initialize optimizers using helper functions\n# optimizer_G = optim.Adam(\n#     get_parameters(model, ['encoder', 'decoder']),\n#     lr=learning_rate, betas=betas\n# )\n# optimizer_D = optim.Adam(\n#     get_parameters(model, ['discriminator']),\n#     lr=learning_rate, betas=betas\n# )\n\n# # ==========================\n# #       LOSS FUNCTIONS\n# # ==========================\n# adversarial_loss = nn.MSELoss().to(device)\n# reconstruction_loss = nn.MSELoss().to(device)\n# kld_loss = lambda mu, logvar: -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n# # ==========================\n# #      MIXED PRECISION\n# # ==========================\n# scaler = GradScaler()\n\n# # ==========================\n# #      TRAINING LOOP\n# # ==========================\n# d_losses = []\n# g_losses = []\n\n# for epoch in tqdm(range(1, epochs + 1), desc=\"Training Epochs\"):\n#     epoch_d_loss = 0\n#     epoch_g_loss = 0\n#     model.train()\n    \n#     for batch_idx, (real_data, _) in enumerate(tqdm(train_loader, desc=\"Batches\", leave=False)):\n#         real_data = real_data.to(device)\n#         batch_size_current = real_data.size(0)\n        \n#         # ===================\n#         #   Train Discriminator\n#         # ===================\n#         optimizer_D.zero_grad()\n#         with autocast():  # Enable autocasting for mixed precision\n#             reconstructed, _, _, _ = model(real_data)\n#             valid = torch.ones(batch_size_current, 1, device=device)\n#             fake = torch.zeros(batch_size_current, 1, device=device)\n            \n#             # Use helper function to access discriminator\n#             discriminator = get_submodule(model, 'discriminator')\n#             real_discrim = discriminator(real_data)\n#             fake_discrim = discriminator(reconstructed.detach())\n            \n#             real_loss = adversarial_loss(real_discrim, valid)\n#             fake_loss = adversarial_loss(fake_discrim, fake)\n#             d_loss = (real_loss + fake_loss) / 2\n#         scaler.scale(d_loss).backward()\n#         scaler.step(optimizer_D)\n#         scaler.update()\n        \n#         # ===================\n#         #    Train Generator\n#         # ===================\n#         optimizer_G.zero_grad()\n#         with autocast():\n#             reconstructed, validity, mu, logvar = model(real_data)\n#             g_adv = adversarial_loss(validity, valid)\n#             g_recon = reconstruction_loss(reconstructed, real_data)\n#             g_kld = kld_loss(mu, logvar)\n#             g_loss = g_adv + g_recon + g_kld\n#         scaler.scale(g_loss).backward()\n#         scaler.step(optimizer_G)\n#         scaler.update()\n        \n#         # Accumulate losses\n#         epoch_d_loss += d_loss.item()\n#         epoch_g_loss += g_loss.item()\n    \n#     # Average losses for the epoch\n#     avg_d_loss = epoch_d_loss / len(train_loader)\n#     avg_g_loss = epoch_g_loss / len(train_loader)\n    \n#     d_losses.append(avg_d_loss)\n#     g_losses.append(avg_g_loss)\n    \n#     # Logging every 100 epochs\n#     if epoch % 100 == 0:\n#         print(f\"Epoch {epoch} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n\n# # ==========================\n# #        SAVE MODEL\n# # ==========================\n# # Save the model\n# if isinstance(model, nn.DataParallel):\n#     torch.save(model.module.state_dict(), 'improved_vae_gan.pth')\n# else:\n#     torch.save(model.state_dict(), 'improved_vae_gan.pth')\n\n# # ==========================\n# #        EVALUATION\n# # ==========================\n# # Parameters for Evaluation\n# batch_size_eval = 1024  # Increased batch size\n\n# # Convert training data to tensors\n# train_dataset_eval = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n# train_loader_eval = DataLoader(\n#     train_dataset_eval,\n#     batch_size=batch_size_eval,\n#     shuffle=False,  # Typically, shuffle=False for evaluation\n#     pin_memory=True if device.type == 'cuda' else False,\n#     num_workers=12  # Increased number of workers for faster data loading\n# )\n\n# # Load trained VAEGAN model and move to device\n# model_eval = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n# if isinstance(model, nn.DataParallel):\n#     model_eval.load_state_dict(torch.load('improved_vae_gan.pth'))\n# else:\n#     model_eval.load_state_dict(torch.load('improved_vae_gan.pth', map_location=device))\n# model_eval.eval()\n# print(\"Model loaded on GPU:\", next(model_eval.parameters()).is_cuda)  # Debug statement\n\n# # Generate synthetic data with tqdm\n# synthetic_data = []\n# with torch.no_grad():\n#     for batch_idx, (real_data, _) in enumerate(tqdm(train_loader_eval, desc=\"Generating Synthetic Data\")):\n#         real_data = real_data.to(device)\n#         mu, logvar = get_submodule(model_eval, 'encoder')(real_data)\n#         z = model_eval.reparameterize(mu, logvar)\n#         gen_data = get_submodule(model_eval, 'decoder')(z)\n#         synthetic_data.append(gen_data.cpu().numpy())\n# synthetic_data = np.vstack(synthetic_data)\n\n# # Combine real and synthetic data\n# X_augmented = np.vstack((X_train, synthetic_data))\n# y_augmented = np.hstack((y_train, np.ones(synthetic_data.shape[0])))\n\n# # Split augmented data\n# X_aug_train, X_aug_test, y_aug_train, y_aug_test = split_data(X_augmented, y_augmented)\n\n# # ==========================\n# #    CLASSIFICATION MODELS\n# # ==========================\n# # Train DNN Classifier\n# dnn = MLPClassifier(hidden_layer_sizes=(200,), max_iter=300, random_state=42)  # Increased hidden layers\n# dnn.fit(X_aug_train, y_aug_train)\n# y_pred_dnn = dnn.predict(X_test)\n\n# # Train XGBoost Classifier\n# xgb_model = xgb.XGBClassifier(\n#     use_label_encoder=False, \n#     eval_metric='logloss', \n#     n_estimators=100, \n#     max_depth=6, \n#     random_state=42\n# )\n# xgb_model.fit(X_train, y_train)\n# y_pred_xgb = xgb_model.predict(X_test)\n\n# # ==========================\n# #          METRICS\n# # ==========================\n# # Metrics for DNN\n# precision_dnn = precision_score(y_test, y_pred_dnn)\n# recall_dnn = recall_score(y_test, y_pred_dnn)\n# f1_dnn = f1_score(y_test, y_pred_dnn)\n\n# # Metrics for XGBoost\n# precision_xgb = precision_score(y_test, y_pred_xgb)\n# recall_xgb = recall_score(y_test, y_pred_xgb)\n# f1_xgb = f1_score(y_test, y_pred_xgb)\n\n# # Print Classification Reports\n# print(\"DNN Classification Report:\")\n# print(classification_report(y_test, y_pred_dnn))\n\n# print(\"XGBoost Classification Report:\")\n# print(classification_report(y_test, y_pred_xgb))\n\n# # Summary of Metrics\n# summary_metrics = {\n#     'DNN': {'Precision': precision_dnn, 'Recall': recall_dnn, 'F1-Score': f1_dnn},\n#     'XGBoost': {'Precision': precision_xgb, 'Recall': recall_xgb, 'F1-Score': f1_xgb}\n# }\n\n# print(\"Summary of Classification Metrics:\")\n# for model_name, metrics in summary_metrics.items():\n#     print(f\"{model_name}: Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, F1-Score={metrics['F1-Score']:.4f}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Even more improved version than above \n\nImprovements Implemented:\r\nFurther Increased Batch Size:\r\n\r\nFrom: batch_size = 1024\r\nTo: batch_size = 4096\r\nThis allows processing more samples in parallel, maximizing GPU utilization across both T4 GPUs.\r\n\r\nFurther Increased Number of DataLoader Workers:\r\n\r\nFrom: num_workers=12\r\nTo: num_workers=24\r\nIncreasing the number of workers enhances data loading speed, ensuring that the GPUs remain active without waiting for data.\r\n\r\nAdjusted Evaluation Parameters:\r\n\r\nIncreased batch_size_eval to 4096\r\nIncreased num_workers to 24\r\nAligning evaluation parameters with training settings ensures consistency and maximizes the efficiency during the synthetic data generation phase.\r\n\r\nEnhanced Discriminator Architecture:\r\n\r\nFurther Increased Layer Sizes in Discriminato","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, TensorDataset\n# from torch.cuda.amp import GradScaler, autocast\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n# from sklearn.neural_network import MLPClassifier\n# import xgboost as xgb\n# import matplotlib.pyplot as plt\n# from tqdm import tqdm\n# import numpy as np\n\n# # ==========================\n# #        PARAMETERS\n# # ==========================\n# data_path = '/kaggle/input/creditcardfraud/creditcard.csv'  # Replace with your dataset path\n# input_dim = 30  # Number of features\n# latent_dim = 100\n# batch_size = 4096  # Increased batch size for GPU utilization\n# epochs = 3000\n# learning_rate = 0.0003\n# betas = (0.5, 0.999)\n\n# # ==========================\n# #    DEVICE CONFIGURATION\n# # ==========================\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# torch.backends.cudnn.benchmark = True  # Enable CuDNN benchmark for performance\n# print(f\"Using device: {device}\")\n\n# # ==========================\n# #      DATA PREPARATION\n# # ==========================\n# def load_data(path):\n#     data = pd.read_csv(path)\n#     return data\n\n# def preprocess_data(data):\n#     # Handle missing values\n#     data = data.dropna()\n    \n#     # Features and target\n#     X = data.drop('Class', axis=1)\n#     y = data['Class']\n    \n#     # Normalize features\n#     scaler = StandardScaler()\n#     X_scaled = scaler.fit_transform(X)\n    \n#     return X_scaled, y\n\n# def split_data(X, y, test_size=0.2, random_state=42):\n#     X_train, X_test, y_train, y_test = train_test_split(\n#         X, y, test_size=test_size, random_state=random_state, stratify=y\n#     )\n#     return X_train, X_test, y_train, y_test\n\n# # Execute preprocessing\n# data = load_data(data_path)\n# X, y = preprocess_data(data)\n# X_train, X_test, y_train, y_test = split_data(X, y)\n\n# # ==========================\n# #      MODEL DEFINITIONS\n# # ==========================\n# # Improved Encoder\n# class ImprovedEncoder(nn.Module):\n#     def __init__(self, input_dim, latent_dim):\n#         super(ImprovedEncoder, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(input_dim, 2048),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, 1024),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, latent_dim * 2)  # Mean and log-variance\n#         )\n    \n#     def forward(self, x):\n#         h = self.fc(x)\n#         mu, logvar = torch.chunk(h, 2, dim=1)\n#         return mu, logvar\n\n# # Improved Decoder\n# class ImprovedDecoder(nn.Module):\n#     def __init__(self, latent_dim, output_dim):\n#         super(ImprovedDecoder, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(latent_dim, 1024),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, 2048),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, output_dim),\n#             nn.Sigmoid()\n#         )\n    \n#     def forward(self, z):\n#         return self.fc(z)\n\n# # Improved Discriminator\n# class ImprovedDiscriminator(nn.Module):\n#     def __init__(self, input_dim):\n#         super(ImprovedDiscriminator, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(input_dim, 4096),  # Further increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(4096, 2048),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, 1),\n#             nn.Sigmoid()\n#         )\n    \n#     def forward(self, x):\n#         return self.fc(x)\n\n# # Improved VAEGAN Model\n# class ImprovedVAEGAN(nn.Module):\n#     def __init__(self, input_dim, latent_dim):\n#         super(ImprovedVAEGAN, self).__init__()\n#         self.encoder = ImprovedEncoder(input_dim, latent_dim)\n#         self.decoder = ImprovedDecoder(latent_dim, input_dim)\n#         self.discriminator = ImprovedDiscriminator(input_dim)\n    \n#     def reparameterize(self, mu, logvar):\n#         std = torch.exp(0.5 * logvar)\n#         eps = torch.randn_like(std)\n#         return mu + eps * std\n    \n#     def forward(self, x):\n#         mu, logvar = self.encoder(x)\n#         z = self.reparameterize(mu, logvar)\n#         reconstructed = self.decoder(z)\n#         validity = self.discriminator(reconstructed)\n#         return reconstructed, validity, mu, logvar\n\n# # ==========================\n# #      HELPER FUNCTIONS\n# # ==========================\n# def get_submodule(model, submodule_name):\n#     \"\"\"\n#     Retrieve the submodule from the model, handling DataParallel if necessary.\n    \n#     Args:\n#         model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n#         submodule_name (str): Name of the submodule to retrieve.\n        \n#     Returns:\n#         torch.nn.Module: The requested submodule.\n#     \"\"\"\n#     if isinstance(model, nn.DataParallel):\n#         return getattr(model.module, submodule_name)\n#     else:\n#         return getattr(model, submodule_name)\n\n# def get_parameters(model, submodules):\n#     \"\"\"\n#     Retrieve the parameters from specified submodules.\n    \n#     Args:\n#         model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n#         submodules (list of str): List of submodule names whose parameters are to be retrieved.\n        \n#     Returns:\n#         list: List of parameters from the specified submodules.\n#     \"\"\"\n#     params = []\n#     for submodule in submodules:\n#         sub = get_submodule(model, submodule)\n#         params += list(sub.parameters())\n#     return params\n\n# # ==========================\n# #      MODEL INITIALIZATION\n# # ==========================\n# # Initialize the model\n# model = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n\n# # Optionally use DataParallel if multiple GPUs are available\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     model = nn.DataParallel(model)\n\n# print(\"Model is on GPU:\", next(model.parameters()).is_cuda)  # Debug statement\n# print(model)  # Print model architecture\n\n# # Function to count total parameters\n# def count_parameters(model):\n#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# total_params = count_parameters(model)\n# print(f\"Total trainable parameters: {total_params}\")\n\n# # ==========================\n# #        DATA LOADING\n# # ==========================\n# # Convert training data to tensors\n# train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n# train_loader = DataLoader(\n#     train_dataset,\n#     batch_size=batch_size,\n#     shuffle=True,\n#     pin_memory=True if device.type == 'cuda' else False,\n#     num_workers=24  # Increased number of workers for faster data loading\n# )\n\n# # ==========================\n# #       OPTIMIZERS\n# # ==========================\n# # Initialize optimizers using helper functions\n# optimizer_G = optim.Adam(\n#     get_parameters(model, ['encoder', 'decoder']),\n#     lr=learning_rate, betas=betas\n# )\n# optimizer_D = optim.Adam(\n#     get_parameters(model, ['discriminator']),\n#     lr=learning_rate, betas=betas\n# )\n\n# # ==========================\n# #       LOSS FUNCTIONS\n# # ==========================\n# adversarial_loss = nn.MSELoss().to(device)\n# reconstruction_loss = nn.MSELoss().to(device)\n# kld_loss = lambda mu, logvar: -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n# # ==========================\n# #      MIXED PRECISION\n# # ==========================\n# scaler = GradScaler()\n\n# # ==========================\n# #      TRAINING LOOP\n# # ==========================\n# d_losses = []\n# g_losses = []\n\n# for epoch in tqdm(range(1, epochs + 1), desc=\"Training Epochs\"):\n#     epoch_d_loss = 0\n#     epoch_g_loss = 0\n#     model.train()\n    \n#     for batch_idx, (real_data, _) in enumerate(tqdm(train_loader, desc=\"Batches\", leave=False)):\n#         real_data = real_data.to(device)\n#         batch_size_current = real_data.size(0)\n        \n#         # ===================\n#         #   Train Discriminator\n#         # ===================\n#         optimizer_D.zero_grad()\n#         with autocast():  # Enable autocasting for mixed precision\n#             reconstructed, _, _, _ = model(real_data)\n#             valid = torch.ones(batch_size_current, 1, device=device)\n#             fake = torch.zeros(batch_size_current, 1, device=device)\n            \n#             # Use helper function to access discriminator\n#             discriminator = get_submodule(model, 'discriminator')\n#             real_discrim = discriminator(real_data)\n#             fake_discrim = discriminator(reconstructed.detach())\n            \n#             real_loss = adversarial_loss(real_discrim, valid)\n#             fake_loss = adversarial_loss(fake_discrim, fake)\n#             d_loss = (real_loss + fake_loss) / 2\n#         scaler.scale(d_loss).backward()\n#         scaler.step(optimizer_D)\n#         scaler.update()\n        \n#         # ===================\n#         #    Train Generator\n#         # ===================\n#         optimizer_G.zero_grad()\n#         with autocast():\n#             reconstructed, validity, mu, logvar = model(real_data)\n#             g_adv = adversarial_loss(validity, valid)\n#             g_recon = reconstruction_loss(reconstructed, real_data)\n#             g_kld = kld_loss(mu, logvar)\n#             g_loss = g_adv + g_recon + g_kld\n#         scaler.scale(g_loss).backward()\n#         scaler.step(optimizer_G)\n#         scaler.update()\n        \n#         # Accumulate losses\n#         epoch_d_loss += d_loss.item()\n#         epoch_g_loss += g_loss.item()\n    \n#     # Average losses for the epoch\n#     avg_d_loss = epoch_d_loss / len(train_loader)\n#     avg_g_loss = epoch_g_loss / len(train_loader)\n    \n#     d_losses.append(avg_d_loss)\n#     g_losses.append(avg_g_loss)\n    \n#     # Logging every 100 epochs\n#     if epoch % 100 == 0:\n#         print(f\"Epoch {epoch} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n\n# # ==========================\n# #        SAVE MODEL\n# # ==========================\n# # Save the model\n# if isinstance(model, nn.DataParallel):\n#     torch.save(model.module.state_dict(), 'improved_vae_gan.pth')\n# else:\n#     torch.save(model.state_dict(), 'improved_vae_gan.pth')\n\n# # ==========================\n# #        EVALUATION\n# # ==========================\n# # Parameters for Evaluation\n# batch_size_eval = 4096  # Further increased batch size\n\n# # Convert training data to tensors\n# train_dataset_eval = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n# train_loader_eval = DataLoader(\n#     train_dataset_eval,\n#     batch_size=batch_size_eval,\n#     shuffle=False,  # Typically, shuffle=False for evaluation\n#     pin_memory=True if device.type == 'cuda' else False,\n#     num_workers=24  # Further increased number of workers for faster data loading\n# )\n\n# # Load trained VAEGAN model and move to device\n# model_eval = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n# if isinstance(model, nn.DataParallel):\n#     model_eval.load_state_dict(torch.load('improved_vae_gan.pth'))\n# else:\n#     model_eval.load_state_dict(torch.load('improved_vae_gan.pth', map_location=device))\n# model_eval.eval()\n# print(\"Model loaded on GPU:\", next(model_eval.parameters()).is_cuda)  # Debug statement\n\n# # Generate synthetic data with tqdm\n# synthetic_data = []\n# with torch.no_grad():\n#     for batch_idx, (real_data, _) in enumerate(tqdm(train_loader_eval, desc=\"Generating Synthetic Data\")):\n#         real_data = real_data.to(device)\n#         mu, logvar = get_submodule(model_eval, 'encoder')(real_data)\n#         z = model_eval.reparameterize(mu, logvar)\n#         gen_data = get_submodule(model_eval, 'decoder')(z)\n#         synthetic_data.append(gen_data.cpu().numpy())\n# synthetic_data = np.vstack(synthetic_data)\n\n# # Combine real and synthetic data\n# X_augmented = np.vstack((X_train, synthetic_data))\n# y_augmented = np.hstack((y_train, np.ones(synthetic_data.shape[0])))\n\n# # Split augmented data\n# X_aug_train, X_aug_test, y_aug_train, y_aug_test = split_data(X_augmented, y_augmented)\n\n# # ==========================\n# #    CLASSIFICATION MODELS\n# # ==========================\n# # Train DNN Classifier\n# dnn = MLPClassifier(hidden_layer_sizes=(200,), max_iter=300, random_state=42)  # Increased hidden layers\n# dnn.fit(X_aug_train, y_aug_train)\n# y_pred_dnn = dnn.predict(X_test)\n\n# # Train XGBoost Classifier\n# xgb_model = xgb.XGBClassifier(\n#     use_label_encoder=False, \n#     eval_metric='logloss', \n#     n_estimators=100, \n#     max_depth=6, \n#     random_state=42\n# )\n# xgb_model.fit(X_train, y_train)\n# y_pred_xgb = xgb_model.predict(X_test)\n\n# # ==========================\n# #          METRICS\n# # ==========================\n# # Metrics for DNN\n# precision_dnn = precision_score(y_test, y_pred_dnn)\n# recall_dnn = recall_score(y_test, y_pred_dnn)\n# f1_dnn = f1_score(y_test, y_pred_dnn)\n\n# # Metrics for XGBoost\n# precision_xgb = precision_score(y_test, y_pred_xgb)\n# recall_xgb = recall_score(y_test, y_pred_xgb)\n# f1_xgb = f1_score(y_test, y_pred_xgb)\n\n# # Print Classification Reports\n# print(\"DNN Classification Report:\")\n# print(classification_report(y_test, y_pred_dnn))\n\n# print(\"XGBoost Classification Report:\")\n# print(classification_report(y_test, y_pred_xgb))\n\n# # Summary of Metrics\n# summary_metrics = {\n#     'DNN': {'Precision': precision_dnn, 'Recall': recall_dnn, 'F1-Score': f1_dnn},\n#     'XGBoost': {'Precision': precision_xgb, 'Recall': recall_xgb, 'F1-Score': f1_xgb}\n# }\n\n# print(\"Summary of Classification Metrics:\")\n# for model_name, metrics in summary_metrics.items():\n#     print(f\"{model_name}: Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, F1-Score={metrics['F1-Score']:.4f}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Even more Advanced code \n\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, TensorDataset\n# from torch.cuda.amp import GradScaler, autocast\n# import pandas as pd\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n# from sklearn.neural_network import MLPClassifier\n# import xgboost as xgb\n# import matplotlib.pyplot as plt\n# from tqdm import tqdm\n# import numpy as np\n\n# # ==========================\n# #        PARAMETERS\n# # ==========================\n# data_path = '/kaggle/input/creditcardfraud/creditcard.csv'  # Replace with your dataset path\n# input_dim = 30  # Number of features\n# latent_dim = 100\n# batch_size = 8192  # Further increased batch size for GPU utilization\n# epochs = 3000\n# learning_rate = 0.0003\n# betas = (0.5, 0.999)\n\n# # ==========================\n# #    DEVICE CONFIGURATION\n# # ==========================\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# torch.backends.cudnn.benchmark = True  # Enable CuDNN benchmark for performance\n# print(f\"Using device: {device}\")\n\n# # ==========================\n# #      DATA PREPARATION\n# # ==========================\n# def load_data(path):\n#     data = pd.read_csv(path)\n#     return data\n\n# def preprocess_data(data):\n#     # Handle missing values\n#     data = data.dropna()\n    \n#     # Features and target\n#     X = data.drop('Class', axis=1)\n#     y = data['Class']\n    \n#     # Normalize features\n#     scaler = StandardScaler()\n#     X_scaled = scaler.fit_transform(X)\n    \n#     return X_scaled, y\n\n# def split_data(X, y, test_size=0.2, random_state=42):\n#     X_train, X_test, y_train, y_test = train_test_split(\n#         X, y, test_size=test_size, random_state=random_state, stratify=y\n#     )\n#     return X_train, X_test, y_train, y_test\n\n# # Execute preprocessing\n# data = load_data(data_path)\n# X, y = preprocess_data(data)\n# X_train, X_test, y_train, y_test = split_data(X, y)\n\n# # ==========================\n# #      MODEL DEFINITIONS\n# # ==========================\n# # Improved Encoder\n# class ImprovedEncoder(nn.Module):\n#     def __init__(self, input_dim, latent_dim):\n#         super(ImprovedEncoder, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(input_dim, 2048),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, 1024),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, latent_dim * 2)  # Mean and log-variance\n#         )\n    \n#     def forward(self, x):\n#         h = self.fc(x)\n#         mu, logvar = torch.chunk(h, 2, dim=1)\n#         return mu, logvar\n\n# # Improved Decoder\n# class ImprovedDecoder(nn.Module):\n#     def __init__(self, latent_dim, output_dim):\n#         super(ImprovedDecoder, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(latent_dim, 1024),  # Increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(1024, 2048),\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, output_dim),\n#             nn.Sigmoid()\n#         )\n    \n#     def forward(self, z):\n#         return self.fc(z)\n\n# # Improved Discriminator\n# class ImprovedDiscriminator(nn.Module):\n#     def __init__(self, input_dim):\n#         super(ImprovedDiscriminator, self).__init__()\n#         self.fc = nn.Sequential(\n#             nn.Linear(input_dim, 4096),  # Further increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(4096, 2048),       # Further increased layer size\n#             nn.ReLU(inplace=True),\n#             nn.Linear(2048, 1),\n#             nn.Sigmoid()\n#         )\n    \n#     def forward(self, x):\n#         return self.fc(x)\n\n# # Improved VAEGAN Model\n# class ImprovedVAEGAN(nn.Module):\n#     def __init__(self, input_dim, latent_dim):\n#         super(ImprovedVAEGAN, self).__init__()\n#         self.encoder = ImprovedEncoder(input_dim, latent_dim)\n#         self.decoder = ImprovedDecoder(latent_dim, input_dim)\n#         self.discriminator = ImprovedDiscriminator(input_dim)\n    \n#     def reparameterize(self, mu, logvar):\n#         std = torch.exp(0.5 * logvar)\n#         eps = torch.randn_like(std)\n#         return mu + eps * std\n    \n#     def forward(self, x):\n#         mu, logvar = self.encoder(x)\n#         z = self.reparameterize(mu, logvar)\n#         reconstructed = self.decoder(z)\n#         validity = self.discriminator(reconstructed)\n#         return reconstructed, validity, mu, logvar\n\n# # ==========================\n# #      HELPER FUNCTIONS\n# # ==========================\n# def get_submodule(model, submodule_name):\n#     \"\"\"\n#     Retrieve the submodule from the model, handling DataParallel if necessary.\n    \n#     Args:\n#         model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n#         submodule_name (str): Name of the submodule to retrieve.\n        \n#     Returns:\n#         torch.nn.Module: The requested submodule.\n#     \"\"\"\n#     if isinstance(model, nn.DataParallel):\n#         return getattr(model.module, submodule_name)\n#     else:\n#         return getattr(model, submodule_name)\n\n# def get_parameters(model, submodules):\n#     \"\"\"\n#     Retrieve the parameters from specified submodules.\n    \n#     Args:\n#         model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n#         submodules (list of str): List of submodule names whose parameters are to be retrieved.\n        \n#     Returns:\n#         list: List of parameters from the specified submodules.\n#     \"\"\"\n#     params = []\n#     for submodule in submodules:\n#         sub = get_submodule(model, submodule)\n#         params += list(sub.parameters())\n#     return params\n\n# # ==========================\n# #      MODEL INITIALIZATION\n# # ==========================\n# # Initialize the model\n# model = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n\n# # Optionally use DataParallel if multiple GPUs are available\n# if torch.cuda.device_count() > 1:\n#     print(f\"Using {torch.cuda.device_count()} GPUs\")\n#     model = nn.DataParallel(model)\n\n# print(\"Model is on GPU:\", next(model.parameters()).is_cuda)  # Debug statement\n# print(model)  # Print model architecture\n\n# # Function to count total parameters\n# def count_parameters(model):\n#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# total_params = count_parameters(model)\n# print(f\"Total trainable parameters: {total_params}\")\n\n# # ==========================\n# #        DATA LOADING\n# # ==========================\n# # Convert training data to tensors\n# train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n# train_loader = DataLoader(\n#     train_dataset,\n#     batch_size=batch_size,\n#     shuffle=True,\n#     pin_memory=True if device.type == 'cuda' else False,\n#     num_workers=48  # Further increased number of workers for faster data loading\n# )\n\n# # ==========================\n# #       OPTIMIZERS\n# # ==========================\n# # Initialize optimizers using helper functions\n# optimizer_G = optim.Adam(\n#     get_parameters(model, ['encoder', 'decoder']),\n#     lr=learning_rate, betas=betas\n# )\n# optimizer_D = optim.Adam(\n#     get_parameters(model, ['discriminator']),\n#     lr=learning_rate, betas=betas\n# )\n\n# # ==========================\n# #       LOSS FUNCTIONS\n# # ==========================\n# adversarial_loss = nn.MSELoss().to(device)\n# reconstruction_loss = nn.MSELoss().to(device)\n# kld_loss = lambda mu, logvar: -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n# # ==========================\n# #      MIXED PRECISION\n# # ==========================\n# scaler = GradScaler()\n\n# # ==========================\n# #      TRAINING LOOP\n# # ==========================\n# d_losses = []\n# g_losses = []\n\n# for epoch in tqdm(range(1, epochs + 1), desc=\"Training Epochs\"):\n#     epoch_d_loss = 0\n#     epoch_g_loss = 0\n#     model.train()\n    \n#     for batch_idx, (real_data, _) in enumerate(tqdm(train_loader, desc=\"Batches\", leave=False)):\n#         real_data = real_data.to(device)\n#         batch_size_current = real_data.size(0)\n        \n#         # ===================\n#         #   Train Discriminator\n#         # ===================\n#         optimizer_D.zero_grad()\n#         with autocast():  # Enable autocasting for mixed precision\n#             reconstructed, _, _, _ = model(real_data)\n#             valid = torch.ones(batch_size_current, 1, device=device)\n#             fake = torch.zeros(batch_size_current, 1, device=device)\n            \n#             # Use helper function to access discriminator\n#             discriminator = get_submodule(model, 'discriminator')\n#             real_discrim = discriminator(real_data)\n#             fake_discrim = discriminator(reconstructed.detach())\n            \n#             real_loss = adversarial_loss(real_discrim, valid)\n#             fake_loss = adversarial_loss(fake_discrim, fake)\n#             d_loss = (real_loss + fake_loss) / 2\n#         scaler.scale(d_loss).backward()\n#         scaler.step(optimizer_D)\n#         scaler.update()\n        \n#         # ===================\n#         #    Train Generator\n#         # ===================\n#         optimizer_G.zero_grad()\n#         with autocast():\n#             reconstructed, validity, mu, logvar = model(real_data)\n#             g_adv = adversarial_loss(validity, valid)\n#             g_recon = reconstruction_loss(reconstructed, real_data)\n#             g_kld = kld_loss(mu, logvar)\n#             g_loss = g_adv + g_recon + g_kld\n#         scaler.scale(g_loss).backward()\n#         scaler.step(optimizer_G)\n#         scaler.update()\n        \n#         # Accumulate losses\n#         epoch_d_loss += d_loss.item()\n#         epoch_g_loss += g_loss.item()\n    \n#     # Average losses for the epoch\n#     avg_d_loss = epoch_d_loss / len(train_loader)\n#     avg_g_loss = epoch_g_loss / len(train_loader)\n    \n#     d_losses.append(avg_d_loss)\n#     g_losses.append(avg_g_loss)\n    \n#     # Logging every 100 epochs\n#     if epoch % 100 == 0:\n#         print(f\"Epoch {epoch} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n\n# # ==========================\n# #        SAVE MODEL\n# # ==========================\n# # Save the model\n# if isinstance(model, nn.DataParallel):\n#     torch.save(model.module.state_dict(), 'improved_vae_gan.pth')\n# else:\n#     torch.save(model.state_dict(), 'improved_vae_gan.pth')\n\n# # ==========================\n# #        EVALUATION\n# # ==========================\n# # Parameters for Evaluation\n# batch_size_eval = 8192  # Further increased batch size\n\n# # Convert training data to tensors\n# train_dataset_eval = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n# train_loader_eval = DataLoader(\n#     train_dataset_eval,\n#     batch_size=batch_size_eval,\n#     shuffle=False,  # Typically, shuffle=False for evaluation\n#     pin_memory=True if device.type == 'cuda' else False,\n#     num_workers=48  # Further increased number of workers for faster data loading\n# )\n\n# # Load trained VAEGAN model and move to device\n# model_eval = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n# if isinstance(model, nn.DataParallel):\n#     model_eval.load_state_dict(torch.load('improved_vae_gan.pth'))\n# else:\n#     model_eval.load_state_dict(torch.load('improved_vae_gan.pth', map_location=device))\n# model_eval.eval()\n# print(\"Model loaded on GPU:\", next(model_eval.parameters()).is_cuda)  # Debug statement\n\n# # Generate synthetic data with tqdm\n# synthetic_data = []\n# with torch.no_grad():\n#     for batch_idx, (real_data, _) in enumerate(tqdm(train_loader_eval, desc=\"Generating Synthetic Data\")):\n#         real_data = real_data.to(device)\n#         mu, logvar = get_submodule(model_eval, 'encoder')(real_data)\n#         z = model_eval.reparameterize(mu, logvar)\n#         gen_data = get_submodule(model_eval, 'decoder')(z)\n#         synthetic_data.append(gen_data.cpu().numpy())\n# synthetic_data = np.vstack(synthetic_data)\n\n# # Combine real and synthetic data\n# X_augmented = np.vstack((X_train, synthetic_data))\n# y_augmented = np.hstack((y_train, np.ones(synthetic_data.shape[0])))\n\n# # Split augmented data\n# X_aug_train, X_aug_test, y_aug_train, y_aug_test = split_data(X_augmented, y_augmented)\n\n# # ==========================\n# #    CLASSIFICATION MODELS\n# # ==========================\n# # Train DNN Classifier\n# dnn = MLPClassifier(hidden_layer_sizes=(200,), max_iter=300, random_state=42)  # Increased hidden layers\n# dnn.fit(X_aug_train, y_aug_train)\n# y_pred_dnn = dnn.predict(X_test)\n\n# # Train XGBoost Classifier\n# xgb_model = xgb.XGBClassifier(\n#     use_label_encoder=False, \n#     eval_metric='logloss', \n#     n_estimators=100, \n#     max_depth=6, \n#     random_state=42\n# )\n# xgb_model.fit(X_train, y_train)\n# y_pred_xgb = xgb_model.predict(X_test)\n\n# # ==========================\n# #          METRICS\n# # ==========================\n# # Metrics for DNN\n# precision_dnn = precision_score(y_test, y_pred_dnn)\n# recall_dnn = recall_score(y_test, y_pred_dnn)\n# f1_dnn = f1_score(y_test, y_pred_dnn)\n\n# # Metrics for XGBoost\n# precision_xgb = precision_score(y_test, y_pred_xgb)\n# recall_xgb = recall_score(y_test, y_pred_xgb)\n# f1_xgb = f1_score(y_test, y_pred_xgb)\n\n# # Print Classification Reports\n# print(\"DNN Classification Report:\")\n# print(classification_report(y_test, y_pred_dnn))\n\n# print(\"XGBoost Classification Report:\")\n# print(classification_report(y_test, y_pred_xgb))\n\n# # Summary of Metrics\n# summary_metrics = {\n#     'DNN': {'Precision': precision_dnn, 'Recall': recall_dnn, 'F1-Score': f1_dnn},\n#     'XGBoost': {'Precision': precision_xgb, 'Recall': recall_xgb, 'F1-Score': f1_xgb}\n# }\n\n# print(\"Summary of Classification Metrics:\")\n# for model_name, metrics in summary_metrics.items():\n#     print(f\"{model_name}: Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, F1-Score={metrics['F1-Score']:.4f}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.cuda.amp import GradScaler, autocast\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\n\n# ==========================\n#        PARAMETERS\n# ==========================\ndata_path = '/kaggle/input/creditcardfraud/creditcard.csv'  # Replace with your dataset path\ninput_dim = 30  # Number of features\nlatent_dim = 100\nbatch_size = 8192  # Further increased batch size for GPU utilization\nepochs = 3000\nlearning_rate = 0.0003\nbetas = (0.5, 0.999)\n\n# ==========================\n#    DEVICE CONFIGURATION\n# ==========================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True  # Enable CuDNN benchmark for performance\nprint(f\"Using device: {device}\")\n\n# ==========================\n#      DATA PREPARATION\n# ==========================\ndef load_data(path):\n    data = pd.read_csv(path)\n    return data\n\ndef preprocess_data(data):\n    # Handle missing values\n    data = data.dropna()\n    \n    # Features and target\n    X = data.drop('Class', axis=1)\n    y = data['Class']\n    \n    # Normalize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    return X_scaled, y\n\ndef split_data(X, y, test_size=0.2, random_state=42):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state, stratify=y\n    )\n    return X_train, X_test, y_train, y_test\n\n# Execute preprocessing\ndata = load_data(data_path)\nX, y = preprocess_data(data)\nX_train, X_test, y_train, y_test = split_data(X, y)\n\n# ==========================\n#      MODEL DEFINITIONS\n# ==========================\n# Improved Encoder\nclass ImprovedEncoder(nn.Module):\n    def __init__(self, input_dim, latent_dim):\n        super(ImprovedEncoder, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 4096),  # Increased layer size\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, 2048),\n            nn.ReLU(inplace=True),\n            nn.Linear(2048, latent_dim * 2)  # Mean and log-variance\n        )\n    \n    def forward(self, x):\n        h = self.fc(x)\n        mu, logvar = torch.chunk(h, 2, dim=1)\n        return mu, logvar\n\n# Improved Decoder\nclass ImprovedDecoder(nn.Module):\n    def __init__(self, latent_dim, output_dim):\n        super(ImprovedDecoder, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(latent_dim, 2048),  # Increased layer size\n            nn.ReLU(inplace=True),\n            nn.Linear(2048, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, output_dim),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, z):\n        return self.fc(z)\n\n# Improved Discriminator\nclass ImprovedDiscriminator(nn.Module):\n    def __init__(self, input_dim):\n        super(ImprovedDiscriminator, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_dim, 8192),  # Further increased layer size\n            nn.ReLU(inplace=True),\n            nn.Linear(8192, 4096),       # Further increased layer size\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return self.fc(x)\n\n# Improved VAEGAN Model\nclass ImprovedVAEGAN(nn.Module):\n    def __init__(self, input_dim, latent_dim):\n        super(ImprovedVAEGAN, self).__init__()\n        self.encoder = ImprovedEncoder(input_dim, latent_dim)\n        self.decoder = ImprovedDecoder(latent_dim, input_dim)\n        self.discriminator = ImprovedDiscriminator(input_dim)\n    \n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def forward(self, x):\n        mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        reconstructed = self.decoder(z)\n        validity = self.discriminator(reconstructed)\n        return reconstructed, validity, mu, logvar\n\n# ==========================\n#      HELPER FUNCTIONS\n# ==========================\ndef get_submodule(model, submodule_name):\n    \"\"\"\n    Retrieve the submodule from the model, handling DataParallel if necessary.\n    \n    Args:\n        model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n        submodule_name (str): Name of the submodule to retrieve.\n        \n    Returns:\n        torch.nn.Module: The requested submodule.\n    \"\"\"\n    if isinstance(model, nn.DataParallel):\n        return getattr(model.module, submodule_name)\n    else:\n        return getattr(model, submodule_name)\n\ndef get_parameters(model, submodules):\n    \"\"\"\n    Retrieve the parameters from specified submodules.\n    \n    Args:\n        model (torch.nn.Module): The main model, possibly wrapped with DataParallel.\n        submodules (list of str): List of submodule names whose parameters are to be retrieved.\n        \n    Returns:\n        list: List of parameters from the specified submodules.\n    \"\"\"\n    params = []\n    for submodule in submodules:\n        sub = get_submodule(model, submodule)\n        params += list(sub.parameters())\n    return params\n\n# ==========================\n#      MODEL INITIALIZATION\n# ==========================\n# Initialize the model\nmodel = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n\n# Optionally use DataParallel if multiple GPUs are available\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model, device_ids=[0,1])\n\nprint(\"Model is on GPU:\", next(model.parameters()).is_cuda)  # Debug statement\nprint(model)  # Print model architecture\n\n# Function to count total parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ntotal_params = count_parameters(model)\nprint(f\"Total trainable parameters: {total_params}\")\n\n# ==========================\n#        DATA LOADING\n# ==========================\n# Convert training data to tensors\ntrain_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    pin_memory=True if device.type == 'cuda' else False,\n    num_workers=48,  # Further increased number of workers for faster data loading\n    prefetch_factor=4  # Added prefetch_factor for better data loading performance\n)\n\n# ==========================\n#       OPTIMIZERS\n# ==========================\n# Initialize optimizers using helper functions\noptimizer_G = optim.Adam(\n    get_parameters(model, ['encoder', 'decoder']),\n    lr=learning_rate, betas=betas\n)\noptimizer_D = optim.Adam(\n    get_parameters(model, ['discriminator']),\n    lr=learning_rate, betas=betas\n)\n\n# ==========================\n#       LOSS FUNCTIONS\n# ==========================\nadversarial_loss = nn.MSELoss().to(device)\nreconstruction_loss = nn.MSELoss().to(device)\nkld_loss = lambda mu, logvar: -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n# ==========================\n#      MIXED PRECISION\n# ==========================\nscaler = GradScaler()\n\n# ==========================\n#      TRAINING LOOP\n# ==========================\nd_losses = []\ng_losses = []\n\nfor epoch in tqdm(range(1, epochs + 1), desc=\"Training Epochs\"):\n    epoch_d_loss = 0\n    epoch_g_loss = 0\n    model.train()\n    \n    for batch_idx, (real_data, _) in enumerate(tqdm(train_loader, desc=\"Batches\", leave=False)):\n        real_data = real_data.to(device)\n        batch_size_current = real_data.size(0)\n        \n        # ===================\n        #   Train Discriminator\n        # ===================\n        optimizer_D.zero_grad()\n        with autocast():  # Enable autocasting for mixed precision\n            reconstructed, _, _, _ = model(real_data)\n            valid = torch.ones(batch_size_current, 1, device=device)\n            fake = torch.zeros(batch_size_current, 1, device=device)\n            \n            # Use helper function to access discriminator\n            discriminator = get_submodule(model, 'discriminator')\n            real_discrim = discriminator(real_data)\n            fake_discrim = discriminator(reconstructed.detach())\n            \n            real_loss = adversarial_loss(real_discrim, valid)\n            fake_loss = adversarial_loss(fake_discrim, fake)\n            d_loss = (real_loss + fake_loss) / 2\n        scaler.scale(d_loss).backward()\n        scaler.step(optimizer_D)\n        scaler.update()\n        \n        # ===================\n        #    Train Generator\n        # ===================\n        optimizer_G.zero_grad()\n        with autocast():\n            reconstructed, validity, mu, logvar = model(real_data)\n            g_adv = adversarial_loss(validity, valid)\n            g_recon = reconstruction_loss(reconstructed, real_data)\n            g_kld = kld_loss(mu, logvar)\n            g_loss = g_adv + g_recon + g_kld\n        scaler.scale(g_loss).backward()\n        scaler.step(optimizer_G)\n        scaler.update()\n        \n        # Accumulate losses\n        epoch_d_loss += d_loss.item()\n        epoch_g_loss += g_loss.item()\n    \n    # Average losses for the epoch\n    avg_d_loss = epoch_d_loss / len(train_loader)\n    avg_g_loss = epoch_g_loss / len(train_loader)\n    \n    d_losses.append(avg_d_loss)\n    g_losses.append(avg_g_loss)\n    \n    # Logging every 100 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n\n# ==========================\n#        SAVE MODEL\n# ==========================\n# Save the model\nif isinstance(model, nn.DataParallel):\n    torch.save(model.module.state_dict(), 'improved_vae_gan.pth')\nelse:\n    torch.save(model.state_dict(), 'improved_vae_gan.pth')\n\n# ==========================\n#        EVALUATION\n# ==========================\n# Parameters for Evaluation\nbatch_size_eval = 8192  # Further increased batch size\n\n# Convert training data to tensors\ntrain_dataset_eval = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\ntrain_loader_eval = DataLoader(\n    train_dataset_eval,\n    batch_size=batch_size_eval,\n    shuffle=False,  # Typically, shuffle=False for evaluation\n    pin_memory=True if device.type == 'cuda' else False,\n    num_workers=48,  # Further increased number of workers for faster data loading\n    prefetch_factor=4  # Added prefetch_factor for better data loading performance\n)\n\n# Load trained VAEGAN model and move to device\nmodel_eval = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\nif isinstance(model, nn.DataParallel):\n    model_eval.load_state_dict(torch.load('improved_vae_gan.pth'))\n    model_eval = nn.DataParallel(model_eval, device_ids=[0,1])\nelse:\n    model_eval.load_state_dict(torch.load('improved_vae_gan.pth', map_location=device))\nprint(\"Model loaded on GPU:\", next(model_eval.parameters()).is_cuda)  # Debug statement\n\n# Generate synthetic data with tqdm\nsynthetic_data = []\nwith torch.no_grad():\n    for batch_idx, (real_data, _) in enumerate(tqdm(train_loader_eval, desc=\"Generating Synthetic Data\")):\n        real_data = real_data.to(device)\n        mu, logvar = get_submodule(model_eval, 'encoder')(real_data)\n        z = model_eval.module.reparameterize(mu, logvar) if isinstance(model_eval, nn.DataParallel) else model_eval.reparameterize(mu, logvar)\n        gen_data = get_submodule(model_eval, 'decoder')(z)\n        synthetic_data.append(gen_data.cpu().numpy())\nsynthetic_data = np.vstack(synthetic_data)\n\n# Combine real and synthetic data\nX_augmented = np.vstack((X_train, synthetic_data))\ny_augmented = np.hstack((y_train, np.ones(synthetic_data.shape[0])))\n\n# Split augmented data\nX_aug_train, X_aug_test, y_aug_train, y_aug_test = split_data(X_augmented, y_augmented)\n\n# ==========================\n#    CLASSIFICATION MODELS\n# ==========================\n# Train DNN Classifier\ndnn = MLPClassifier(hidden_layer_sizes=(200,), max_iter=300, random_state=42)  # Increased hidden layers\ndnn.fit(X_aug_train, y_aug_train)\ny_pred_dnn = dnn.predict(X_test)\n\n# Train XGBoost Classifier\nxgb_model = xgb.XGBClassifier(\n    use_label_encoder=False, \n    eval_metric='logloss', \n    n_estimators=100, \n    max_depth=6, \n    random_state=42\n)\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\n\n# ==========================\n#          METRICS\n# ==========================\n# Metrics for DNN\nprecision_dnn = precision_score(y_test, y_pred_dnn)\nrecall_dnn = recall_score(y_test, y_pred_dnn)\nf1_dnn = f1_score(y_test, y_pred_dnn)\n\n# Metrics for XGBoost\nprecision_xgb = precision_score(y_test, y_pred_xgb)\nrecall_xgb = recall_score(y_test, y_pred_xgb)\nf1_xgb = f1_score(y_test, y_pred_xgb)\n\n# Print Classification Reports\nprint(\"DNN Classification Report:\")\nprint(classification_report(y_test, y_pred_dnn))\n\nprint(\"XGBoost Classification Report:\")\nprint(classification_report(y_test, y_pred_xgb))\n\n# Summary of Metrics\nsummary_metrics = {\n    'DNN': {'Precision': precision_dnn, 'Recall': recall_dnn, 'F1-Score': f1_dnn},\n    'XGBoost': {'Precision': precision_xgb, 'Recall': recall_xgb, 'F1-Score': f1_xgb}\n}\n\nprint(\"Summary of Classification Metrics:\")\nfor model_name, metrics in summary_metrics.items():\n    print(f\"{model_name}: Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, F1-Score={metrics['F1-Score']:.4f}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Improved code --> latest code with features like \n'''\n\n1.Add Attention Mechanisms:\n    Implement self-attention layers in the encoder and decoder to enhance feature representation.\n2.Implement Residual Connections:\n    Incorporate residual blocks within the encoder and decoder to facilitate deeper architectures.\n3.Explore Different Fusion Strategies:\n    Replace simple averaging with concatenation followed by a dense layer for merging encoder outputs.\n4.Introduce Dropout Layers:\n    Add dropout layers to prevent overfitting in the encoder, decoder, and discriminator.\n5.Use Alternative Loss Functions:\n    Integrate Wasserstein loss with gradient penalty for improved GAN training stability.\n6.Integrate Batch Normalization:\n    Apply batch normalization after linear layers to stabilize and accelerate training.\n7.Expand Latent Space Dimensionality:\n    Increase the latent dimension to capture more complex data distributions.\n8.Incorporate Conditional GANs:\n    Add label conditioning to the GAN to generate controlled synthetic samples.\n9.Enhance Data Augmentation:\n    Apply advanced data augmentation techniques like SMOTE or ADASYN.\n10.Optimize Training Strategies:\n    Use AdamW optimizer and implement learning rate schedulers for better convergence.\n11.Implement Early Stopping and Checkpointing:\n    Add early stopping to prevent overfitting and save the best model checkpoints.\n12.Evaluate with Diverse Metrics:\n    Include ROC-AUC and Precision-Recall AUC in the evaluation metrics.\n13.Parallelize Data Processing:\n    Optimize DataLoader settings to fully utilize CPU cores.\n14.Leverage Transfer Learning:\n    Utilize a pre-trained model for the discriminator to benefit from learned features.\n15.Apply Gradient Penalty:\n    Incorporate gradient penalty in the discriminator loss to improve GAN training stability.\n\n'''\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.cuda.amp import GradScaler, autocast\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report, roc_auc_score, precision_recall_curve\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom models.encoder import ImprovedEncoder\nfrom models.decoder import ImprovedDecoder\nfrom models.discriminator import ImprovedDiscriminator\nfrom utils.early_stopping import EarlyStopping\n\n# ==========================\n#        PARAMETERS\n# ==========================\ndata_path = '/kaggle/input/creditcardfraud/creditcard.csv'  # Replace with your dataset path\ninput_dim = 30  # Number of features\nlatent_dim = 200  # Expanded latent dimension\nnum_classes = 2  # For conditional GAN\nbatch_size = 8192\nepochs = 3000\nlearning_rate = 0.0003\nbetas = (0.5, 0.999)\npatience = 50  # For early stopping\n\n# ==========================\n#    DEVICE CONFIGURATION\n# ==========================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\nprint(f\"Using device: {device}\")\n\n# ==========================\n#      DATA PREPARATION\n# ==========================\ndef load_data(path):\n    data = pd.read_csv(path)\n    return data\n\ndef preprocess_data(data):\n    data = data.dropna()\n    X = data.drop('Class', axis=1)\n    y = data['Class']\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    return X_scaled, y\n\ndef split_data(X, y, test_size=0.2, random_state=42):\n    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n\ndata = load_data(data_path)\nX, y = preprocess_data(data)\nX_train, X_test, y_train, y_test = split_data(X, y)\n\n# ==========================\n#      DATA AUGMENTATION\n# ==========================\nfrom imblearn.over_sampling import ADASYN\n\ndef augment_data(X, y):\n    sampler = ADASYN(random_state=42)\n    X_res, y_res = sampler.fit_resample(X, y)\n    return X_res, y_res\n\nX_train_aug, y_train_aug = augment_data(X_train, y_train)\n\n# ==========================\n#      MODEL DEFINITIONS\n# ==========================\nmodel = ImprovedVAEGAN(input_dim=input_dim, latent_dim=latent_dim).to(device)\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)\n\nprint(\"Model is on GPU:\", next(model.parameters()).is_cuda)\nprint(model)\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Total trainable parameters: {count_parameters(model)}\")\n\n# ==========================\n#        DATA LOADING\n# ==========================\ntrain_dataset = TensorDataset(torch.FloatTensor(X_train_aug), torch.LongTensor(y_train_aug))\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    pin_memory=True if device.type == 'cuda' else False,\n    num_workers=16,\n    prefetch_factor=2\n)\n\n# ==========================\n#       OPTIMIZERS & SCHEDULERS\n# ==========================\noptimizer_G = optim.AdamW(\n    list(model.module.encoder.parameters()) + list(model.module.decoder.parameters()),\n    lr=learning_rate, betas=betas\n)\noptimizer_D = optim.AdamW(\n    model.module.discriminator.parameters(),\n    lr=learning_rate, betas=betas\n)\n\nscheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=1000, gamma=0.5)\nscheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=1000, gamma=0.5)\n\n# ==========================\n#       LOSS FUNCTIONS\n# ==========================\nadversarial_loss = nn.BCELoss().to(device)\nreconstruction_loss = nn.MSELoss().to(device)\n\ndef gradient_penalty(discriminator, real_samples, fake_samples, labels):\n    alpha = torch.randn(real_samples.size(0), 1).to(device)\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = discriminator(interpolates, labels)\n    fake = torch.ones(real_samples.size(0), 1).to(device)\n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True\n    )[0]\n    gradients = gradients.view(gradients.size(0), -1)\n    penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return penalty\n\n# ==========================\n#      MIXED PRECISION\n# ==========================\nscaler = GradScaler()\n\n# ==========================\n#   EARLY STOPPING\n# ==========================\nearly_stopping = EarlyStopping(patience=patience, verbose=True)\n\n# ==========================\n#      TRAINING LOOP\n# ==========================\nd_losses = []\ng_losses = []\n\nfor epoch in tqdm(range(1, epochs + 1), desc=\"Training Epochs\"):\n    epoch_d_loss = 0\n    epoch_g_loss = 0\n    model.train()\n    \n    for batch_idx, (real_data, labels) in enumerate(tqdm(train_loader, desc=\"Batches\", leave=False)):\n        real_data = real_data.to(device)\n        labels = labels.to(device)\n        batch_size_current = real_data.size(0)\n        \n        # ===================\n        #   Train Discriminator\n        # ===================\n        optimizer_D.zero_grad()\n        with autocast():\n            reconstructed, _, mu, logvar = model(real_data)\n            valid = torch.ones(batch_size_current, 1, device=device)\n            fake = torch.zeros(batch_size_current, 1, device=device)\n            \n            real_validity = model.module.discriminator(real_data, labels)\n            fake_validity = model.module.discriminator(reconstructed.detach(), labels)\n            \n            d_real_loss = adversarial_loss(real_validity, valid)\n            d_fake_loss = adversarial_loss(fake_validity, fake)\n            gp = gradient_penalty(model.module.discriminator, real_data, reconstructed, labels)\n            d_loss = d_real_loss + d_fake_loss + 10 * gp\n        scaler.scale(d_loss).backward()\n        scaler.step(optimizer_D)\n        scaler.update()\n        \n        # ===================\n        #    Train Generator\n        # ===================\n        optimizer_G.zero_grad()\n        with autocast():\n            reconstructed, validity, mu, logvar = model(real_data)\n            g_adv = adversarial_loss(validity, valid)\n            g_recon = reconstruction_loss(reconstructed, real_data)\n            g_kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n            g_loss = g_adv + g_recon + g_kld\n        scaler.scale(g_loss).backward()\n        scaler.step(optimizer_G)\n        scaler.update()\n        \n        epoch_d_loss += d_loss.item()\n        epoch_g_loss += g_loss.item()\n    \n    scheduler_D.step()\n    scheduler_G.step()\n    \n    avg_d_loss = epoch_d_loss / len(train_loader)\n    avg_g_loss = epoch_g_loss / len(train_loader)\n    \n    d_losses.append(avg_d_loss)\n    g_losses.append(avg_g_loss)\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n    \n    # Early Stopping\n    early_stopping(avg_g_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered\")\n        break\n\n# ==========================\n#        SAVE MODEL\n# ==========================\ntorch.save(model.module.state_dict(), 'improved_vae_gan.pth')\n\n# ==========================\n#        EVALUATION\n# ==========================\nmodel.eval()\nsynthetic_data = []\nsynthetic_labels = []\n\nwith torch.no_grad():\n    for real_data, labels in tqdm(train_loader, desc=\"Generating Synthetic Data\"):\n        real_data = real_data.to(device)\n        labels = labels.to(device)\n        mu, logvar = model.module.encoder(real_data)\n        z = model.module.reparameterize(mu, logvar)\n        gen_data = model.module.decoder(z)\n        synthetic_data.append(gen_data.cpu().numpy())\n        synthetic_labels.append(labels.cpu().numpy())\n\nsynthetic_data = np.vstack(synthetic_data)\nsynthetic_labels = np.hstack(synthetic_labels)\n\nX_augmented = np.vstack((X_train, synthetic_data))\ny_augmented = np.hstack((y_train, synthetic_labels))\n\nX_aug_train, X_aug_test, y_aug_train, y_aug_test = split_data(X_augmented, y_augmented)\n\n# ==========================\n#    CLASSIFICATION MODELS\n# ==========================\n# Train DNN Classifier\ndnn = MLPClassifier(hidden_layer_sizes=(200,), max_iter=300, random_state=42)\ndnn.fit(X_aug_train, y_aug_train)\ny_pred_dnn = dnn.predict(X_test)\ny_prob_dnn = dnn.predict_proba(X_test)[:,1]\n\n# Train XGBoost Classifier\nxgb_model = xgb.XGBClassifier(\n    use_label_encoder=False, \n    eval_metric='logloss', \n    n_estimators=100, \n    max_depth=6, \n    random_state=42\n)\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\ny_prob_xgb = xgb_model.predict_proba(X_test)[:,1]\n\n# ==========================\n#          METRICS\n# ==========================\n# Metrics for DNN\nprecision_dnn = precision_score(y_test, y_pred_dnn)\nrecall_dnn = recall_score(y_test, y_pred_dnn)\nf1_dnn = f1_score(y_test, y_pred_dnn)\nroc_auc_dnn = roc_auc_score(y_test, y_prob_dnn)\n\n# Metrics for XGBoost\nprecision_xgb = precision_score(y_test, y_pred_xgb)\nrecall_xgb = recall_score(y_test, y_pred_xgb)\nf1_xgb = f1_score(y_test, y_pred_xgb)\nroc_auc_xgb = roc_auc_score(y_test, y_prob_xgb)\n\n# Print Classification Reports\nprint(\"DNN Classification Report:\")\nprint(classification_report(y_test, y_pred_dnn))\n\nprint(\"XGBoost Classification Report:\")\nprint(classification_report(y_test, y_pred_xgb))\n\n# Summary of Metrics\nsummary_metrics = {\n    'DNN': {'Precision': precision_dnn, 'Recall': recall_dnn, 'F1-Score': f1_dnn, 'ROC-AUC': roc_auc_dnn},\n    'XGBoost': {'Precision': precision_xgb, 'Recall': recall_xgb, 'F1-Score': f1_xgb, 'ROC-AUC': roc_auc_xgb}\n}\n\nprint(\"Summary of Classification Metrics:\")\nfor model_name, metrics in summary_metrics.items():\n    print(f\"{model_name}: Precision={metrics['Precision']:.4f}, Recall={metrics['Recall']:.4f}, F1-Score={metrics['F1-Score']:.4f}, ROC-AUC={metrics['ROC-AUC']:.4f}\")\n    \n# Plot ROC Curves\nfrom sklearn.metrics import roc_curve\n\nfpr_dnn, tpr_dnn, _ = roc_curve(y_test, y_prob_dnn)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)\n\nplt.figure(figsize=(10,7))\nplt.plot(fpr_dnn, tpr_dnn, label=f'DNN ROC AUC = {roc_auc_dnn:.4f}')\nplt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost ROC AUC = {roc_auc_xgb:.4f}')\nplt.plot([0,1], [0,1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves')\nplt.legend()\nplt.show()\n\n# Plot Precision-Recall Curves\nprecision_dnn_vals, recall_dnn_vals, _ = precision_recall_curve(y_test, y_prob_dnn)\nprecision_xgb_vals, recall_xgb_vals, _ = precision_recall_curve(y_test, y_prob_xgb)\n\nplt.figure(figsize=(10,7))\nplt.plot(recall_dnn_vals, precision_dnn_vals, label=f'DNN PR AUC = {roc_auc_dnn:.4f}')\nplt.plot(recall_xgb_vals, precision_xgb_vals, label=f'XGBoost PR AUC = {roc_auc_xgb:.4f}')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curves')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}